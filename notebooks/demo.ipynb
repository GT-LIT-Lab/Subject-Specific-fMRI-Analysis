{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demo uses LANGUAGE task data from a sample of 30 subjects in the Human Connectome Project (HCP) Young Adult dataset to showcase the processing pipeline. This includes first-level processing through to a variety of fROI-based analyses, such as effect size, spatial correlations and spatial overlap estimations.\n",
    "\n",
    "The language localizer task in the HCP involves two conditions: a story condition, where participants listen to brief auditory stories followed by a comprehension question, and a math condition, where participants solve arithmetic problems. These tasks are designed to activate distinct regions of the brain, with the story condition engaging the language network and the math condition serving as a non-linguistic control. fMRI data collected during these tasks allow researchers to identify brain regions specifically involved in language processing.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "Before running the demo locally, please configure your AWS credentials to access the HCP dataset. Follow these steps:\n",
    "\n",
    "1. Refer to the [HCP wiki guide](https://wiki.humanconnectome.org/docs/How%20To%20Connect%20to%20Connectome%20Data%20via%20AWS.html) for instructions on obtaining AWS credentials for accessing the dataset.\n",
    "    \n",
    "2. Configure and store your credentials in the `~/.aws/credentials` file. You can find detailed instructions in the [AWS CLI user guide](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\n",
    "    \"211417\", \"164030\", \"480141\", \"248238\", \"214221\", \"381038\", \"117021\", \n",
    "    \"671855\", \"352738\", \"180836\", \"677968\", \"200917\", \"715647\", \"107018\", \n",
    "    \"937160\", \"349244\", \"214625\", \"286347\", \"715041\", \"749058\", \"614439\", \n",
    "    \"250932\", \"145834\", \"872158\", \"164636\", \"932554\", \"118528\", \"737960\", \n",
    "    \"187547\", \"110613\"\n",
    "]\n",
    "subjects_heldout = [\n",
    "    '996782', '995174', '994273', '993675', '992774', '992673', '991267',\n",
    "    '990366', '989987', '987983'\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funROI.datasets import hcp\n",
    "hcp.fetch_language_data(\"./data\", subjects + subjects_heldout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Level Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first-level model in fMRI processing is designed to analyze individual subject data by modeling the relationship between task-related experimental conditions and the observed brain activity, by constructing a General Linear Model (GLM) for each voxel to estimate condition-specific effects and identify brain regions activated by the task.\n",
    "\n",
    "The funROI toolbox wraps Nilearn's first-level modeling, supporting event-related and block designs, customizable hemodynamic response functions, confound regression, and statistical contrasts. Below, we demonstrate how to configure and run a first-level model using funROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funROI\n",
    "funROI.set_bids_data_folder('./data/bids')\n",
    "funROI.set_bids_preprocessed_folder('./data/bids') # using HCP preprocessed data\n",
    "funROI.set_bids_deriv_folder('./data/bids/derivatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from funROI.first_level.nilearn import run_first_level\n",
    "run_first_level(\n",
    "    task = 'LANGUAGE',\n",
    "    subjects = subjects + subjects_heldout,\n",
    "    space = 'MNINonLinear',\n",
    "    contrasts = [\n",
    "        ('story', {'story': 1}),\n",
    "        ('math', {'math': 1}),\n",
    "        ('story-math', {'story': 1, 'math': -1}),\n",
    "    ],\n",
    "    slice_time_ref = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Parcels for the Language System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will demonstrate how to generate parcels for the language system using the 30-subject sample. We will focus on the story-math contrast to isolate regions of the brain involved in language processing. These group-level parcels will later serve as spatial constraints for defining subject-specific functional regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funROI.set_analysis_output_folder(\"./data/analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funROI.analysis import ParcelsGenerator\n",
    "parcels_generator = ParcelsGenerator(\n",
    "    parcels_name=\"Language\",\n",
    "    smoothing_kernel_size=8,\n",
    "    overlap_thr_vox=0.05,\n",
    "    min_voxel_size=100,\n",
    "    overlap_thr_roi=0.8\n",
    ")\n",
    "parcels_generator.add_subjects(\n",
    "    subjects=subjects,\n",
    "    task=\"LANGUAGE\",\n",
    "    contrasts=[\"story-math\"],\n",
    "    p_threshold_type=\"none\",\n",
    "    p_threshold_value=0.05,\n",
    ")\n",
    "parcels = parcels_generator.run(return_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the parcels generated using a sample of 30 subjects for the language system.\n",
    "The code snippet below plots the parcels on the brain surface for better visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_surf_roi\n",
    "from nilearn.datasets import fetch_surf_fsaverage\n",
    "from nilearn.surface import vol_to_surf\n",
    "fsaverage = fetch_surf_fsaverage('fsaverage5')\n",
    "\n",
    "surf_data = {\n",
    "    \"left\": vol_to_surf(parcels, fsaverage.pial_left, interpolation='nearest', radius=0),\n",
    "    \"right\": vol_to_surf(parcels, fsaverage.pial_right, interpolation='nearest', radius=0),\n",
    "}\n",
    "\n",
    "views = [\"lateral\", \"medial\"]\n",
    "hemispheres = [\"left\", \"right\"]\n",
    "\n",
    "for hemi in hemispheres:\n",
    "    for view in views:\n",
    "        plot_surf_roi(\n",
    "            surf_mesh=getattr(fsaverage, f\"pial_{hemi}\"),\n",
    "            roi_map=surf_data[hemi],\n",
    "            hemi=hemi,\n",
    "            view=view,\n",
    "            bg_on_data=True,\n",
    "            bg_map=getattr(fsaverage, f\"sulc_{hemi}\"),\n",
    "            darkness=0.5,\n",
    "            cmap=\"gist_rainbow\",\n",
    "            avg_method='max',\n",
    "            title=f\"{hemi.capitalize()} Hemisphere - {view.capitalize()} View\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendered results:\n",
    "\n",
    "<img src=\"images/parcels_left_lateral.png\" width=300 />\n",
    "<img src=\"images/parcels_right_lateral.png\" width=300 />\n",
    "<br>\n",
    "<img src=\"images/parcels_left_medial.png\" width=300 />\n",
    "<img src=\"images/parcels_right_medial.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Effect Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect size estimation is a critical step in fROI analysis, as it provides a quantitative measure of the strength of the neural response to specific contrasts or conditions.\n",
    "\n",
    "In this section, we will demonstrate how to estimate effect sizes for story and math conditions within subjects' language fROIs. We will use the language system parcel generated in the previous section. For each parcel, the language fROI is defined as the top 10% of voxels responding to the story-math contrast. The effect sizes of the language system will be evaluated in the defined fROIs.\n",
    "\n",
    "The analyses below will be done using an heldout set of 10 subjects independent from subjects for generating language parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funROI.analysis import EffectEstimator\n",
    "froi = funROI.FROIConfig(\n",
    "    task=\"LANGUAGE\",\n",
    "    contrasts=[\"story-math\"],\n",
    "    threshold_type=\"percent\",\n",
    "    threshold_value=0.1,\n",
    "    parcels=\"./data/analysis/parcels/Language/Language_0000.nii.gz\",\n",
    ")\n",
    "effect_estimator = EffectEstimator(subjects=subjects_heldout, froi=froi)\n",
    "df_summary, df_detail = effect_estimator.run(\n",
    "    task=\"LANGUAGE\", effects=[\"story\", \"math\"], return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,5))\n",
    "data = df_summary.groupby([\"subject\", \"effect\"]).mean().reset_index()\n",
    "sns.barplot(data=data, y=\"size\", x=\"effect\", hue=\"effect\", errorbar=\"se\")\n",
    "plt.ylabel(\"Effect Size\")\n",
    "plt.xlabel(\"Effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we examine the effect sizes for the subjects' language system, we observe the expected higher responsiveness to story compared to math. This confirms the validity of our approach. \n",
    "\n",
    "<img src=\"images/effect_size.png\" width=300 height=500 />\n",
    "\n",
    "More interesting questions can be explored by applying the language localizer to evaluate response magnitude for other conditions in other task runs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Spatial Correlation Across Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial correlation provides a valuable metric for assessing the similarity of activation patterns across different conditions or runs. This analysis can be performed on parcels or fROIs, allowing researchers to evaluate the consistency of functional responses in specific regions of the brain.\n",
    "\n",
    "In the context of the HCP dataset, which includes only two runs for the language localizer task, we cannot fully utilize fROIs for spatial correlation due to the lack of sufficient runs (at least three are required). However, we can demonstrate spatial correlation between the story and math conditions using the previously defined parcels. When datasets with more runs are available, this approach can be extended to fROIs for a more refined analysis of spatial similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funROI.analysis import SpatialCorrelationEstimator\n",
    "spcorr_estimator = SpatialCorrelationEstimator(\n",
    "    subjects=subjects_heldout,\n",
    "    froi=\"./data/analysis/parcels/Language/Language_0000.nii.gz\"\n",
    ")\n",
    "df_math, _ = spcorr_estimator.run(\n",
    "    task1='LANGUAGE', effect1='math', task2='LANGUAGE', effect2='math',\n",
    "    return_results=True\n",
    ")\n",
    "df_story, _ = spcorr_estimator.run(\n",
    "    task1='LANGUAGE', effect1='story', task2='LANGUAGE', effect2='story',\n",
    "    return_results=True\n",
    ")\n",
    "df_between, _ = spcorr_estimator.run(\n",
    "    task1='LANGUAGE', effect1='story', task2='LANGUAGE', effect2='math',\n",
    "    return_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math['Type'] = 'Math-Math'\n",
    "df_story['Type'] = 'Story-Story'\n",
    "df_between['Type'] = 'Story-Math'\n",
    "data = pd.concat(\n",
    "    [df_between, df_math, df_story]\n",
    ").groupby([\"subject\", \"Type\"]).mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.barplot(data=data, y=\"fisher_z\", x=\"Type\", hue=\"Type\", errorbar=\"se\")   \n",
    "plt.ylabel(\"Fishers Z Correlation\")\n",
    "plt.xlabel(\"Comparison Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendered results:\n",
    "\n",
    "<img src=\"images/spatial_correlation.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Overlap Between fROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the spatial overlap between parcels or fROIs can be useful in various ways, guiding comparisons across tasks and conditions. In this section, we showcase an example of computing the degree of overlap between the language system defined using the localizer, as stated above (10% top voxels), across subjects and within subjects using different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funROI.analysis import OverlapEstimator\n",
    "overlap_estimator = OverlapEstimator()\n",
    "\n",
    "data = []\n",
    "for i, subject1 in enumerate(subjects_heldout):\n",
    "    df, _ = overlap_estimator.run(\n",
    "        froi1=froi, froi2=froi, subject1=subject1, subject2=subject1,\n",
    "        return_results=True\n",
    "    )\n",
    "    data.append(df[df['froi1'] == df['froi2']])\n",
    "\n",
    "    subject2 = subjects_heldout[(i+1) % len(subjects_heldout)]\n",
    "    df, _ = overlap_estimator.run(\n",
    "        froi1=froi, froi2=froi, subject1=subject1, subject2=subject2,\n",
    "        return_results=True\n",
    "    )\n",
    "    data.append(df[df['froi1'] == df['froi2']])\n",
    "\n",
    "data = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['subject1'] == data['subject2'], 'Type'] = 'Within'\n",
    "data.loc[data['subject1'] != data['subject2'], 'Type'] = 'Between'\n",
    "data_mean = data.groupby([\"subject1\", \"subject2\", \"Type\"]).mean().reset_index()\n",
    "plt.figure(figsize=(3,5))\n",
    "sns.barplot(data=data_mean, y=\"overlap\", x=\"Type\", hue=\"Type\", errorbar=\"se\")\n",
    "plt.ylabel(\"Overlap\")\n",
    "plt.xlabel(\"Comparison Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results visualized below illustrate the spatial overlap results:\n",
    "\n",
    "<img src=\"images/overlap.png\" width=300 height=500 />\n",
    "\n",
    "They demonstrate that the within-subject definitions are more consistent compared to across-subject definitions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funROI",
   "language": "python",
   "name": "funroi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
