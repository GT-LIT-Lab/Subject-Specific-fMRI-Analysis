{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showcases a demo using this toolbox to analyze data on HCP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the LANGUAGE task data from a sample of 30 subjects in the HCP dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_objects(s3_client, bucket_name, prefix):\n",
    "    bucket = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    return [obj['Key'] for obj in bucket.get('Contents', [])]\n",
    "\n",
    "\n",
    "def download_file(s3_client, bucket_name, s3_key, local_path):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        s3_client.download_file(bucket_name, s3_key, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Missing or failed: {s3_key}\")\n",
    "\n",
    "\n",
    "def download_selected(s3_client, subjects, task):\n",
    "    patterns = [\n",
    "        \"MNINonLinear/Results/tfMRI_{task}_LR\",\n",
    "        \"MNINonLinear/Results/tfMRI_{task}_RL\"\n",
    "    ]\n",
    "\n",
    "    for subject in subjects:\n",
    "        for pattern in patterns:\n",
    "            s3_path = f\"HCP_1200/{subject}/{pattern.format(task=task)}\"\n",
    "            s3_objects = list_s3_objects(s3_client, 'hcp-openaccess', s3_path)\n",
    "            for s3_object in s3_objects:\n",
    "                download_file(s3_client, 'hcp-openaccess', s3_object, \n",
    "                              'data/' + s3_object)\n",
    "\n",
    "\n",
    "aws_access_key_id = input(\"Enter your AWS access key ID: \")\n",
    "aws_secret_access_key = input(\"Enter your AWS secret access key: \")\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "subjects = [\n",
    "    \"211417\", \"164030\", \"480141\", \"248238\", \"214221\", \"381038\", \"117021\", \"671855\", \n",
    "    \"352738\", \"180836\", \"677968\", \"200917\", \"715647\", \"107018\", \"937160\", \"349244\", \n",
    "    \"214625\", \"286347\", \"715041\", \"749058\", \"614439\", \"250932\", \"145834\", \"872158\", \n",
    "    \"164636\", \"932554\", \"118528\", \"737960\", \"187547\", \"110613\"\n",
    "]\n",
    "task = 'LANGUAGE'\n",
    "\n",
    "download_selected(s3_client, subjects, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The few lines of codes below tidy the HCP dataset in BIDS format. TEST TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "bids_dir = 'bids'\n",
    "\n",
    "subject_task_info = pd.read_csv('./HCP_summary/inclusion_results.csv')\n",
    "subjs = []\n",
    "for i, row in subject_task_info.iterrows():\n",
    "    # check empty tasks\n",
    "    tasks = ast.literal_eval(row['tasks'])\n",
    "    if len(tasks) == 0:\n",
    "        continue\n",
    "    subjs.append(str(row['subject']))\n",
    "\n",
    "def get_events(ev_folder_path, task):\n",
    "    events_df = pd.DataFrame(columns=[\"onset\", \"duration\", \"trial_type\", \"amplitude\"])\n",
    "    for ev_file in glob.glob(f\"{ev_folder_path}/*.txt\"):\n",
    "        if 'Sync' in ev_file: continue\n",
    "        condition = ev_file.split('/')[-1].replace('.txt', '')\n",
    "        ev_data = pd.read_csv(ev_file, sep=\"\\t\", header=None, names=[\"onset\", \"duration\", \"amplitude\"])\n",
    "        ev_data[\"trial_type\"] = condition\n",
    "        \n",
    "        events_df = pd.concat([events_df, ev_data], ignore_index=True)\n",
    "    events_df = events_df[[\"onset\", \"duration\", \"trial_type\", \"amplitude\"]]\n",
    "    events_df = events_df.sort_values(\"onset\").reset_index(drop=True)\n",
    "    events_df = events_df.loc[events_df['trial_type'].isin['story', 'math']][['trial_type', 'onset', 'duration']]\n",
    "    return events_df\n",
    "\n",
    "for sub in subjs:\n",
    "    print(f\"Processing {sub}\")\n",
    "    runs = os.listdir(os.path.join(data_dir, sub, 'MNINonLinear', 'Results'))\n",
    "    for run in [run for run in runs if 'tfMRI' in run]:\n",
    "        run_task = run.split('_')[1]\n",
    "\n",
    "        run_suffix = run.split('_')[-1]\n",
    "        tab_file = glob.glob(\n",
    "            os.path.join(data_dir, sub, 'MNINonLinear', 'Results', run, f'LANGUAGE*_TAB.txt')\n",
    "        )\n",
    "        if len(tab_file) != 1:\n",
    "            raise ValueError(f\"Expected 1 tab file, found {len(tab_file)}\")\n",
    "        tab_file = tab_file[0]\n",
    "        label = os.path.basename(tab_file).split('_TAB')[0]\n",
    "        run_i = label.split('_')[-1].replace('run', '')\n",
    "        \n",
    "        folder = os.path.join(data_dir, sub, 'MNINonLinear', 'Results', run)\n",
    "        bids_folder = os.path.join(bids_dir, f'sub-{sub}', 'func')\n",
    "        pathlib.Path(bids_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # JSON file\n",
    "        dest = os.path.join(bids_folder, f'sub-{sub}_task-{run_task}_run-{run_i}_space-MNINonLinear_desc-preproc_bold.json')\n",
    "        os.symlink(os.path.join(bids_dir, f'task-{run_task}_acq-{run_suffix}_bold.json'), dest)\n",
    "\n",
    "        # Confounds file\n",
    "        tsv_no_header = os.path.join(data_dir, sub, 'MNINonLinear', 'Results', run, 'Movement_Regressors.txt')\n",
    "        columns = [\n",
    "            'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z',\n",
    "            'trans_dx', 'trans_dy', 'trans_dz', 'rot_dx', 'rot_dy', 'rot_dz']\n",
    "        data = []\n",
    "        with open(tsv_no_header, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                data.append([float(x) for x in line])\n",
    "        tbl = pd.DataFrame(data, columns=columns)\n",
    "        tbl.to_csv(os.path.join(bids_dir, f'sub-{sub}', 'func', f'sub-{sub}_task-{run_task}_run-{run_i}_desc-confounds_timeseries.tsv'), sep='\\t', index=False)\n",
    "\n",
    "        filename = run.split('_', 1)[1]\n",
    "        func_dir = os.path.join(bids_dir, f'sub-{sub}', 'func')\n",
    "        os.makedirs(func_dir, exist_ok=True)\n",
    "\n",
    "        # Brain mask file\n",
    "        brain_mask_dest = os.path.join(func_dir, f'sub-{sub}_task-{run_task}_run-{run_i}_space-MNINonLinear_desc-brain_mask.nii.gz')\n",
    "        os.symlink(os.path.join(folder, 'brainmask_fs.2.nii.gz'), brain_mask_dest)\n",
    "\n",
    "        # Preprocessed BOLD file\n",
    "        preproc_bold_dest = os.path.join(func_dir, f'sub-{sub}_task-{run_task}_run-{run_i}_space-MNINonLinear_desc-preproc_bold.nii.gz')\n",
    "        os.symlink(os.path.join(folder, f'tfMRI_{filename}.nii.gz'), preproc_bold_dest)\n",
    "\n",
    "        evs_folder = os.path.join(data_dir, sub, 'MNINonLinear', 'Results', run, 'EVs')\n",
    "        events_df = get_events(evs_folder, run_task)\n",
    "        events_df.to_csv(os.path.join(bids_dir, f'sub-{sub}', 'func', f'sub-{sub}_task-{run_task}_run-{run_i}_events.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Level Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Parcels for the Language Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Individual fROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Estimate Effect Sizs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Estimate Spatial Correlation Across Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Estimate Overlap Between fROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "funroi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
